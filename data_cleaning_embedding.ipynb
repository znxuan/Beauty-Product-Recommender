{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import tqdm\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "import hashlib\n",
    "load_in_4bit=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data by joining parent asin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged 701528 reviews into 'merged_reviews.json'.\n"
     ]
    }
   ],
   "source": [
    "# Load metadata\n",
    "meta_dict = {}\n",
    "with open(\"meta_All_beauty.jsonl\", \"r\") as meta_file:\n",
    "    for line in meta_file:\n",
    "        data = json.loads(line)\n",
    "        parent_asin = data.get(\"parent_asin\")\n",
    "        if parent_asin:\n",
    "            meta_dict[parent_asin] = {\n",
    "                \"title\": data.get(\"title\"),\n",
    "                \"average_rating\": data.get(\"average_rating\"),\n",
    "                \"rating_number\": data.get(\"rating_number\"),\n",
    "            }\n",
    "\n",
    "# Load reviews data and process reviews and merge\n",
    "merged_reviews = []\n",
    "with open(\"All_beauty.jsonl\", \"r\") as review_file:\n",
    "    for line in review_file:\n",
    "        review = json.loads(line)\n",
    "        parent_asin = review.get(\"parent_asin\")\n",
    "        if parent_asin in meta_dict:\n",
    "            merged_reviews.append({\n",
    "                \"parent_asin\": parent_asin,\n",
    "                \"asin\": review.get(\"asin\"),\n",
    "                \"title\": meta_dict[parent_asin][\"title\"],\n",
    "                \"average_rating\": meta_dict[parent_asin][\"average_rating\"],\n",
    "                \"rating_number\": meta_dict[parent_asin][\"rating_number\"],\n",
    "                \"rating\": review.get(\"rating\"),\n",
    "                \"text\": review.get(\"text\"),\n",
    "            })\n",
    "\n",
    "# Save merged data\n",
    "with open(\"merged_reviews.json\", \"w\") as output_file:\n",
    "    json.dump(merged_reviews, output_file, indent=4)\n",
    "\n",
    "print(f\"Merged {len(merged_reviews)} reviews into 'merged_reviews.json'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Data to 15000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset saved to filtered_reviews.jsonl with 15000 reviews.\n"
     ]
    }
   ],
   "source": [
    "# Load merged data\n",
    "input_file = \"merged_reviews.json\"\n",
    "reviews = []\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    reviews = json.load(f)  # Load entire JSON array\n",
    "\n",
    "# Group reviews by rating\n",
    "rating_groups = defaultdict(list)\n",
    "for review in reviews:\n",
    "    rating = int(review[\"average_rating\"])  # Convert to integer (e.g., 4.0 -> 4)\n",
    "    rating_groups[rating].append(review)\n",
    "\n",
    "# Define how many reviews to take per rating\n",
    "total_reviews = 15000\n",
    "num_ratings = len(rating_groups)\n",
    "reviews_per_rating = total_reviews // num_ratings  # Equal distribution\n",
    "\n",
    "# Sample reviews\n",
    "filtered_reviews = []\n",
    "for rating, group in rating_groups.items():\n",
    "    sample_size = min(reviews_per_rating, len(group))  # Avoid exceeding available data\n",
    "    filtered_reviews.extend(random.sample(group, sample_size))\n",
    "\n",
    "# Save filtered reviews\n",
    "output_file = \"filtered_reviews.jsonl\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for review in filtered_reviews:\n",
    "        f.write(json.dumps(review) + \"\\n\")\n",
    "\n",
    "print(f\"Filtered dataset saved to {output_file} with {len(filtered_reviews)} reviews.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Removed 1 reviews with invalid 'title'\n",
      " Remaining reviews: 14999\n",
      " Cleaned reviews saved to 'filtered_reviews.jsonl'\n"
     ]
    }
   ],
   "source": [
    "# Remove reviews with invalid or missing product titles\n",
    "cleaned_reviews = [\n",
    "    review for review in filtered_reviews\n",
    "    if review.get(\"title\") and review.get(\"title\").strip().lower() != \"n/a\"\n",
    "]\n",
    "\n",
    "# Summary\n",
    "removed_count = len(filtered_reviews) - len(cleaned_reviews)\n",
    "print(f\" Removed {removed_count} reviews with invalid 'title'\")\n",
    "print(f\" Remaining reviews: {len(cleaned_reviews)}\")\n",
    "\n",
    "# Save\n",
    "output_cleaned_file = \"filtered_reviews.jsonl\"\n",
    "with open(output_cleaned_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for review in cleaned_reviews:\n",
    "        f.write(json.dumps(review) + \"\\n\")\n",
    "\n",
    "print(f\" Cleaned reviews saved to '{output_cleaned_file}'\")\n"
   ]
  },
 { 
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Chunk Product Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/beauty/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Initialize Semantic Chunker with percentile threshold\n",
    "hf_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-MiniLM-L3-v2\")\n",
    "text_splitter = SemanticChunker(\n",
    "    hf_embeddings,\n",
    "    breakpoint_threshold_type=\"percentile\"  # Smart dynamic chunking\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSONL Review Data\n",
    "data = []\n",
    "with open(\"filtered_reviews.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        json_obj = json.loads(line)\n",
    "        data.append(json_obj)\n",
    "\n",
    "# Process & chunk reviews with tqdm progress bar\n",
    "chunks = []\n",
    "print(\" Chunking reviews with SemanticChunker (percentile-based)...\")\n",
    "for review in tqdm(data, desc=\"Chunking\"):\n",
    "    if len(review[\"text\"].split()) > 100:\n",
    "\n",
    "        review_text = review[\"text\"]\n",
    "        product_name = review.get(\"title\", \"\")\n",
    "        avg_rating = review.get(\"average_rating\", 0)\n",
    "        rating_number = review.get(\"rating_number\", 0)\n",
    "\n",
    "        try:\n",
    "            split_docs = text_splitter.create_documents([review_text])\n",
    "\n",
    "            for doc in split_docs:\n",
    "                chunks.append({\n",
    "                    \"chunk\": doc.page_content,\n",
    "                    \"average_rating\": avg_rating,\n",
    "                    \"rating_number\": rating_number,\n",
    "                    \"product\": product_name\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing review: {product_name} â€” {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_to_store = []\n",
    "for chunk in chunks:\n",
    "    doc = Document(\n",
    "    page_content=f\"Product: {chunk['product']}\\nReview: {chunk['chunk']}\",\n",
    "    metadata={\n",
    "        \"product\": chunk[\"product\"],\n",
    "        \"average_rating\": chunk[\"average_rating\"],\n",
    "        \"rating_number\": chunk[\"rating_number\"]\n",
    "    }\n",
    ")\n",
    "    documents_to_store.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store them into ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1644 unique chunks saved to Chroma DB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lj/1xrbj6bj2qx5x946zx5hm6qh0000gn/T/ipykernel_24051/3692310378.py:25: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "def hash_chunk(text):\n",
    "    return hashlib.md5(text.strip().lower().encode()).hexdigest()\n",
    "\n",
    "# Deduplicate based on hash of page_content\n",
    "unique_chunks = {}\n",
    "for doc in documents_to_store:\n",
    "    h = hash_chunk(doc.page_content)\n",
    "    if h not in unique_chunks:\n",
    "        unique_chunks[h] = doc  # Keep only unique chunk by content\n",
    "\n",
    "# Store only the deduplicated values\n",
    "unique_documents = list(unique_chunks.values())\n",
    "\n",
    "persist_directory = \"chromadb_reviews\"\n",
    "\n",
    "# Save to Chroma\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=unique_documents,\n",
    "    embedding=hf_embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "vectordb.persist()\n",
    "print(f\"{len(unique_documents)} unique chunks saved to Chroma DB.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beauty",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
